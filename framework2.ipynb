{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from keras import Input\n",
    "from eval_utils import calculate_results, prepare_for_evaluation\n",
    "from dataloader import *\n",
    "from  config_translator import *\n",
    "from data_processor import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = DataLoader(config_mixed_coco14_coco14_glove)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data=preprocess_data(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ModelImpl:\n",
    "    def __init__(self, data):\n",
    "        self.data=data\n",
    "        LSTM_NODES=256\n",
    "\n",
    "        embedding_layer = Embedding(self.data.num_words_inputs,glove[\"embedings_dim\"], weights=[self.data.embedding_matrix_input], input_length=self.data.max_input_len)\n",
    "        encoder_inputs = Input(shape=(self.data.max_input_len,))\n",
    "        x = embedding_layer(encoder_inputs)\n",
    "        encoder = LSTM(LSTM_NODES, return_state=True)\n",
    "        encoder_outputs, h, c = encoder(x)\n",
    "        encoder_states = [h, c]\n",
    "        # The next step is to define the decoder. The decoder will have two inputs: the hidden state and cell state from the encoder and the input sentence, which actually will be the output sentence with an <sos> token appended at the beginning.\n",
    "\n",
    "        decoder_inputs = Input(shape=(self.data.max_output_len,))\n",
    "        decoder_embedding = Embedding(self.data.num_words_output, LSTM_NODES)\n",
    "        decoder_inputs_x = decoder_embedding(decoder_inputs)\n",
    "        decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)\n",
    "        # Finally, the output from the decoder LSTM is passed through a dense layer to predict decoder outputs.\n",
    "        decoder_dense = Dense(self.data.num_words_output, activation='softmax')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        self.encoder_model = Model([encoder_inputs,decoder_inputs], decoder_outputs)\n",
    "        self.encoder_model.compile(\n",
    "            optimizer=self.optimizer(),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        self.encoder_model.summary()\n",
    "\n",
    "        def decoder_model(decoder_embedding,decoder_lstm,decoder_dense,LSTM_NODES=256):\n",
    "            decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
    "            decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
    "            decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "            decoder_inputs_single = Input(shape=(1,))\n",
    "            decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "            decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
    "            decoder_states = [h, c]\n",
    "            decoder_outputs = decoder_dense(decoder_outputs)\n",
    "            decoder_model = Model([decoder_inputs_single] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "            return decoder_model\n",
    "\n",
    "        self.decoder_model = decoder_model(decoder_embedding,decoder_lstm,decoder_dense,LSTM_NODES=256)\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        # model.optimizer.lr = 0.001\n",
    "        self.epochs = 100\n",
    "        self.batch_size=64\n",
    "        number_pics_per_bath = 3\n",
    "        self.steps = len(self.data.encoder_input_sequences) // number_pics_per_bath\n",
    "\n",
    "    def optimizer(self):\n",
    "        return Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        model_weights_path=\"./\" + self.data.configuration[\"data_name\"] + self.data.configuration[\"model_save_dir\"]\n",
    "        if self.data.configuration[\"train_model\"]:\n",
    "            es = EarlyStopping(monitor='loss', min_delta=0.001, patience=3)\n",
    "            filepath = model_weights_path + self.data.configuration[\"model_save_path\"]\n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True,\n",
    "                                         mode='min', save_weights_only=False)\n",
    "            callbacks_list = [checkpoint, es, CSVLogger(\"./\" + self.data.configuration[\"data_name\"] +'/logs.csv', separator=\",\", append=True),]\n",
    "            if self.data.configuration[\"continue_training\"]:\n",
    "                self.encoder_model = load_model(filepath)\n",
    "                print(\"New model loaded\")\n",
    "\n",
    "            self.encoder_model.fit([self.data.encoder_input_sequences, self.data.decoder_input_sequences], self.data.decoder_targets_one_hot,\n",
    "                epochs=self.epochs,\n",
    "                steps_per_epoch=self.steps,\n",
    "                callbacks=[callbacks_list],\n",
    "                verbose=1,\n",
    "            )\n",
    "            if self.data.configuration[\"save_model\"]:\n",
    "                writepath = model_weights_path+ \"/\"+'model' + '.h5'\n",
    "                self.encoder_model.save(writepath)\n",
    "                self.encoder_model.save_weights(model_weights_path\n",
    "                                        + self.data.configuration[\"model_save_path\"])\n",
    "        else:\n",
    "            self.encoder_model.load_weights(model_weights_path\n",
    "                                        +self.data.configuration[\"model_save_path\"])\n",
    "\n",
    "    def evaluate(self):\n",
    "        expected, results = prepare_for_evaluation(self.data.encoder_test_sequences, self.data.test_captions_mapping,self.encoder_model, self.decoder_model, self.word2idx_outputs\n",
    "                                                   , self.data.max_output_len)\n",
    "        out = calculate_results(expected, results, self.data.configuration)\n",
    "        print(out)\n",
    "model=ModelImpl(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}