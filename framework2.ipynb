{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from keras import Input\n",
    "from eval_utils import calculate_results, prepare_for_evaluation, generate_report\n",
    "from dataloader import *\n",
    "from  config_translator import *\n",
    "from data_processor import preprocess_data\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = DataLoader(config_mixed_coco14_coco14_glove)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data=preprocess_data(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines, padding_type='post'):\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    X = pad_sequences(X, maxlen=length, padding=padding_type)\n",
    "    return X\n",
    "\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for seq in sequences:\n",
    "        encoded = to_categorical(seq, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = np.array(ylist)\n",
    "    y = y.reshape((sequences.shape[0], sequences.shape[1], vocab_size))\n",
    "    return y\n",
    "\n",
    "def data_generator(train_input, input_tokenizer, max_input_length, train_output, output_tokenizer, max_output_length,\n",
    "                   output_vocab_size, batch_size=64):\n",
    "    while 1:\n",
    "        count = 0\n",
    "\n",
    "        while 1:\n",
    "            if count >= len(train_output):\n",
    "                count = 0\n",
    "\n",
    "                input_seq = list()\n",
    "                output_seq = list()\n",
    "                for i in range(count, min(len(train_input), count+batch_size)):\n",
    "                    input, output = train_input[i], train_output[i]\n",
    "                    input_seq.append(input)\n",
    "                    output_seq.append(output)\n",
    "                input_seq = encode_sequences(input_tokenizer, max_input_length, input_seq)\n",
    "                output_seq = encode_sequences(output_tokenizer, max_output_length, output_seq)\n",
    "                output_seq = encode_output(output_seq, output_vocab_size)\n",
    "\n",
    "                count = count + batch_size\n",
    "\n",
    "                input_seq = np.array(input_seq)\n",
    "                output_seq = np.array(output_seq)\n",
    "                output_seq = output_seq.reshape((output_seq.shape[0], output_seq.shape[1],output_vocab_size))\n",
    "                import tensorflow as tf\n",
    "                import numpy as np\n",
    "                with tf.device('/cpu:0'):\n",
    "                    input_seq = tf.convert_to_tensor(input_seq, np.float32)\n",
    "                    output_seq = tf.convert_to_tensor(output_seq, np.float32)\n",
    "                yield [input_seq, output_seq]\n",
    "\n",
    "class ModelImpl:\n",
    "    def __init__(self, data):\n",
    "        self.data=data\n",
    "        def define_model_glove(input_vocab_size, output_vocab_size, max_input_length, max_output_length, n_units,\n",
    "                               embedding_matrix, embedings_dim):\n",
    "            model = Sequential()\n",
    "\n",
    "            emb = Embedding(input_vocab_size, embedings_dim, input_length=max_input_length, weights=[embedding_matrix], mask_zero=True)\n",
    "            model.add(emb)\n",
    "            lstm_lay = LSTM(n_units)\n",
    "            model.add(lstm_lay)\n",
    "            model.add(RepeatVector(max_output_length))\n",
    "            model.add(LSTM(n_units, return_sequences=True))\n",
    "            model.add(TimeDistributed(Dense(output_vocab_size, activation='softmax')))\n",
    "            return model\n",
    "\n",
    "        self.model = define_model_glove(self.data.input_vocab_size, self.data.output_vocab_size, self.data.max_input_length,\n",
    "                           self.data.max_output_length, 256, self.data.embedding_matrix_input, glove[\"embedings_dim\"])\n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer(),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        self.model.summary()\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        # model.optimizer.lr = 0.001\n",
    "        self.epochs = 100\n",
    "        self.batch_size=64\n",
    "        number_pics_per_bath = 3\n",
    "        self.steps = len(self.data.encoded_images_train) // number_pics_per_bath\n",
    "\n",
    "    def optimizer(self):\n",
    "        return Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    def train(self):\n",
    "        model_weights_path=\"./\" + self.data.configuration[\"data_name\"] + self.data.configuration[\"model_save_dir\"]\n",
    "        if self.data.configuration[\"train_model\"]:\n",
    "            es = EarlyStopping(monitor='loss', min_delta=0.001, patience=3)\n",
    "            filepath = model_weights_path + self.data.configuration[\"model_save_path\"]\n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True,\n",
    "                                         mode='min', save_weights_only=False)\n",
    "            callbacks_list = [checkpoint, es, CSVLogger(\"./\" + self.data.configuration[\"data_name\"] +'/logs.csv', separator=\",\", append=True),]\n",
    "            if self.data.configuration[\"continue_training\"]:\n",
    "                self.model = load_model(filepath)\n",
    "                print(\"New model loaded\")\n",
    "\n",
    "            train_generator = data_generator(self.data.train_bbox_categories_list, self.data.input_tokenizer,\n",
    "                                             self.data.max_input_length,self.data.train_output_sentences_list,\n",
    "                                             self.data.output_tokenizer,self.data.max_output_length,\n",
    "                                             self.data.output_vocab_size, batch_size= self.batch_size)\n",
    "            self.model.fit_generator(train_generator,\n",
    "                epochs=self.epochs,\n",
    "                steps_per_epoch=self.steps,\n",
    "                callbacks=[callbacks_list],\n",
    "                verbose=1,\n",
    "            )\n",
    "            if self.data.configuration[\"save_model\"]:\n",
    "                writepath = model_weights_path+ \"/\"+'model' + '.h5'\n",
    "                self.model.save(writepath)\n",
    "                self.model.save_weights(model_weights_path\n",
    "                                        + self.data.configuration[\"model_save_path\"])\n",
    "        else:\n",
    "            self.model.load_weights(model_weights_path\n",
    "                                        +self.data.configuration[\"model_save_path\"])\n",
    "\n",
    "    def evaluate(self):\n",
    "        expected, results = prepare_for_evaluation(self.data.encoder_test_sequences, self.data.test_captions_mapping,\n",
    "                                                   model, self.decoder_model, self.data.word2idx_outputs\n",
    "                                                   , self.data.max_output_len)\n",
    "        out = calculate_results(expected, results, self.data.configuration)\n",
    "        print(out)\n",
    "model=ModelImpl(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_report(general[\"results_directory\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}